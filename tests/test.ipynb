{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b36e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss = 2.328439\n",
      "Epoch 020 | Loss = 2.292909\n",
      "Epoch 040 | Loss = 2.259031\n",
      "Epoch 060 | Loss = 2.226400\n",
      "Epoch 080 | Loss = 2.194966\n",
      "Epoch 100 | Loss = 2.164567\n",
      "Epoch 120 | Loss = 2.134936\n",
      "Epoch 140 | Loss = 2.105931\n",
      "Epoch 160 | Loss = 2.077378\n",
      "Epoch 180 | Loss = 2.049208\n",
      "Epoch 200 | Loss = 2.021272\n",
      "Epoch 220 | Loss = 1.993637\n",
      "Epoch 240 | Loss = 1.966237\n",
      "Epoch 260 | Loss = 1.938925\n",
      "Epoch 280 | Loss = 1.911729\n",
      "Epoch 300 | Loss = 1.884428\n",
      "Epoch 320 | Loss = 1.857241\n",
      "Epoch 340 | Loss = 1.830107\n",
      "Epoch 360 | Loss = 1.803139\n",
      "Epoch 380 | Loss = 1.776078\n",
      "Epoch 400 | Loss = 1.749096\n",
      "Epoch 420 | Loss = 1.722062\n",
      "Epoch 440 | Loss = 1.695002\n",
      "Epoch 460 | Loss = 1.668011\n",
      "Epoch 480 | Loss = 1.641140\n",
      "Epoch 500 | Loss = 1.614376\n",
      "Epoch 520 | Loss = 1.587682\n",
      "Epoch 540 | Loss = 1.561051\n",
      "Epoch 560 | Loss = 1.534616\n",
      "Epoch 580 | Loss = 1.508415\n",
      "Epoch 600 | Loss = 1.482432\n",
      "Epoch 620 | Loss = 1.456657\n",
      "Epoch 640 | Loss = 1.431062\n",
      "Epoch 660 | Loss = 1.405566\n",
      "Epoch 680 | Loss = 1.380179\n",
      "Epoch 700 | Loss = 1.355072\n",
      "Epoch 720 | Loss = 1.330158\n",
      "Epoch 740 | Loss = 1.305482\n",
      "Epoch 760 | Loss = 1.281052\n",
      "Epoch 780 | Loss = 1.256849\n",
      "Epoch 800 | Loss = 1.232939\n",
      "Epoch 820 | Loss = 1.209387\n",
      "Epoch 840 | Loss = 1.186140\n",
      "Epoch 860 | Loss = 1.163238\n",
      "Epoch 880 | Loss = 1.140674\n",
      "Epoch 900 | Loss = 1.118475\n",
      "Epoch 920 | Loss = 1.096659\n",
      "Epoch 940 | Loss = 1.075242\n",
      "Epoch 960 | Loss = 1.054205\n",
      "Epoch 980 | Loss = 1.033517\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pymatcha.tensor import Tensor\n",
    "from pymatcha.nn import Linear, ReLU, Softmax\n",
    "from pymatcha.optim import SGD, CrossEntropyLoss\n",
    "\n",
    "# -------------------------\n",
    "# 测试训练小网络（分类任务）\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. 构造分类数据集\n",
    "    # -------------------------\n",
    "    num_samples = 200       # 样本数\n",
    "    num_features = 20       # 输入维度\n",
    "    num_classes = 10        # 分类类别数\n",
    "\n",
    "    # 输入特征\n",
    "    X_data = np.random.randn(num_samples, num_features)\n",
    "\n",
    "    # 权重矩阵用于生成类别分界\n",
    "    true_w = np.random.randn(num_features, num_classes)\n",
    "\n",
    "    # 生成 logits（每个样本对应 num_classes 个分数）\n",
    "    logits = X_data @ true_w\n",
    "\n",
    "    # 取最大分数的索引作为“真实类别标签”\n",
    "    y_label = np.argmax(logits, axis=1)  # shape = (num_samples,)\n",
    "\n",
    "    # 转成 Tensor\n",
    "    X = Tensor(X_data, requires_grad=False)\n",
    "    y_true = Tensor(y_label, requires_grad=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. 定义小型网络\n",
    "    # -------------------------\n",
    "    fc1 = Linear(num_features, 64)\n",
    "    relu = ReLU()\n",
    "    fc2 = Linear(64, num_classes)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(fc1.parameters() + fc2.parameters(), lr=0.01)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. 训练循环\n",
    "    # -------------------------\n",
    "    for epoch in range(1000):\n",
    "        # 前向传播\n",
    "        h = relu(fc1(X))\n",
    "        y_pred = softmax(fc2(h))\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # 反向传播 + 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | Loss = {loss.data:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ababb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss = 20.520117\n",
      "Epoch 01 | Loss = 20.393260\n",
      "Epoch 02 | Loss = 20.267414\n",
      "Epoch 03 | Loss = 20.142484\n",
      "Epoch 04 | Loss = 20.018380\n",
      "Epoch 05 | Loss = 19.895014\n",
      "Epoch 06 | Loss = 19.772303\n",
      "Epoch 07 | Loss = 19.650166\n",
      "Epoch 08 | Loss = 19.528527\n",
      "Epoch 09 | Loss = 19.407311\n",
      "Epoch 10 | Loss = 19.286447\n",
      "Epoch 11 | Loss = 19.165868\n",
      "Epoch 12 | Loss = 19.045507\n",
      "Epoch 13 | Loss = 18.925303\n",
      "Epoch 14 | Loss = 18.805194\n",
      "Epoch 15 | Loss = 18.685123\n",
      "Epoch 16 | Loss = 18.565036\n",
      "Epoch 17 | Loss = 18.444880\n",
      "Epoch 18 | Loss = 18.324603\n",
      "Epoch 19 | Loss = 18.204159\n",
      "Epoch 20 | Loss = 18.083502\n",
      "Epoch 21 | Loss = 17.962588\n",
      "Epoch 22 | Loss = 17.841377\n",
      "Epoch 23 | Loss = 17.719828\n",
      "Epoch 24 | Loss = 17.597907\n",
      "Epoch 25 | Loss = 17.475579\n",
      "Epoch 26 | Loss = 17.352810\n",
      "Epoch 27 | Loss = 17.229572\n",
      "Epoch 28 | Loss = 17.105836\n",
      "Epoch 29 | Loss = 16.981577\n",
      "Epoch 30 | Loss = 16.856772\n",
      "Epoch 31 | Loss = 16.731399\n",
      "Epoch 32 | Loss = 16.605440\n",
      "Epoch 33 | Loss = 16.478876\n",
      "Epoch 34 | Loss = 16.351695\n",
      "Epoch 35 | Loss = 16.223884\n",
      "Epoch 36 | Loss = 16.095432\n",
      "Epoch 37 | Loss = 15.966331\n",
      "Epoch 38 | Loss = 15.836576\n",
      "Epoch 39 | Loss = 15.706164\n",
      "Epoch 40 | Loss = 15.575093\n",
      "Epoch 41 | Loss = 15.443364\n",
      "Epoch 42 | Loss = 15.310980\n",
      "Epoch 43 | Loss = 15.177946\n",
      "Epoch 44 | Loss = 15.044271\n",
      "Epoch 45 | Loss = 14.909963\n",
      "Epoch 46 | Loss = 14.775035\n",
      "Epoch 47 | Loss = 14.639501\n",
      "Epoch 48 | Loss = 14.503376\n",
      "Epoch 49 | Loss = 14.366680\n",
      "Epoch 50 | Loss = 14.229431\n",
      "Epoch 51 | Loss = 14.091652\n",
      "Epoch 52 | Loss = 13.953368\n",
      "Epoch 53 | Loss = 13.814604\n",
      "Epoch 54 | Loss = 13.675388\n",
      "Epoch 55 | Loss = 13.535750\n",
      "Epoch 56 | Loss = 13.395721\n",
      "Epoch 57 | Loss = 13.255334\n",
      "Epoch 58 | Loss = 13.114623\n",
      "Epoch 59 | Loss = 12.973625\n",
      "Epoch 60 | Loss = 12.832377\n",
      "Epoch 61 | Loss = 12.690917\n",
      "Epoch 62 | Loss = 12.549287\n",
      "Epoch 63 | Loss = 12.407526\n",
      "Epoch 64 | Loss = 12.265677\n",
      "Epoch 65 | Loss = 12.123784\n",
      "Epoch 66 | Loss = 11.981889\n",
      "Epoch 67 | Loss = 11.840039\n",
      "Epoch 68 | Loss = 11.698277\n",
      "Epoch 69 | Loss = 11.556651\n",
      "Epoch 70 | Loss = 11.415206\n",
      "Epoch 71 | Loss = 11.273989\n",
      "Epoch 72 | Loss = 11.133046\n",
      "Epoch 73 | Loss = 10.992425\n",
      "Epoch 74 | Loss = 10.852173\n",
      "Epoch 75 | Loss = 10.712335\n",
      "Epoch 76 | Loss = 10.572959\n",
      "Epoch 77 | Loss = 10.434091\n",
      "Epoch 78 | Loss = 10.295776\n",
      "Epoch 79 | Loss = 10.158060\n",
      "Epoch 80 | Loss = 10.020987\n",
      "Epoch 81 | Loss = 9.884600\n",
      "Epoch 82 | Loss = 9.748944\n",
      "Epoch 83 | Loss = 9.614060\n",
      "Epoch 84 | Loss = 9.479989\n",
      "Epoch 85 | Loss = 9.346772\n",
      "Epoch 86 | Loss = 9.214447\n",
      "Epoch 87 | Loss = 9.083052\n",
      "Epoch 88 | Loss = 8.952623\n",
      "Epoch 89 | Loss = 8.823197\n",
      "Epoch 90 | Loss = 8.694806\n",
      "Epoch 91 | Loss = 8.567484\n",
      "Epoch 92 | Loss = 8.441262\n",
      "Epoch 93 | Loss = 8.316168\n",
      "Epoch 94 | Loss = 8.192232\n",
      "Epoch 95 | Loss = 8.069480\n",
      "Epoch 96 | Loss = 7.947937\n",
      "Epoch 97 | Loss = 7.827626\n",
      "Epoch 98 | Loss = 7.708570\n",
      "Epoch 99 | Loss = 7.590789\n",
      "Epoch 100 | Loss = 7.474302\n",
      "Epoch 101 | Loss = 7.359126\n",
      "Epoch 102 | Loss = 7.245278\n",
      "Epoch 103 | Loss = 7.132771\n",
      "Epoch 104 | Loss = 7.021618\n",
      "Epoch 105 | Loss = 6.911831\n",
      "Epoch 106 | Loss = 6.803419\n",
      "Epoch 107 | Loss = 6.696391\n",
      "Epoch 108 | Loss = 6.590753\n",
      "Epoch 109 | Loss = 6.486513\n",
      "Epoch 110 | Loss = 6.383672\n",
      "Epoch 111 | Loss = 6.282236\n",
      "Epoch 112 | Loss = 6.182206\n",
      "Epoch 113 | Loss = 6.083581\n",
      "Epoch 114 | Loss = 5.986362\n",
      "Epoch 115 | Loss = 5.890547\n",
      "Epoch 116 | Loss = 5.796133\n",
      "Epoch 117 | Loss = 5.703115\n",
      "Epoch 118 | Loss = 5.611490\n",
      "Epoch 119 | Loss = 5.521251\n",
      "Epoch 120 | Loss = 5.432392\n",
      "Epoch 121 | Loss = 5.344906\n",
      "Epoch 122 | Loss = 5.258783\n",
      "Epoch 123 | Loss = 5.174014\n",
      "Epoch 124 | Loss = 5.090591\n",
      "Epoch 125 | Loss = 5.008502\n",
      "Epoch 126 | Loss = 4.927736\n",
      "Epoch 127 | Loss = 4.848282\n",
      "Epoch 128 | Loss = 4.770127\n",
      "Epoch 129 | Loss = 4.693259\n",
      "Epoch 130 | Loss = 4.617665\n",
      "Epoch 131 | Loss = 4.543330\n",
      "Epoch 132 | Loss = 4.470242\n",
      "Epoch 133 | Loss = 4.398386\n",
      "Epoch 134 | Loss = 4.327746\n",
      "Epoch 135 | Loss = 4.258310\n",
      "Epoch 136 | Loss = 4.190060\n",
      "Epoch 137 | Loss = 4.122983\n",
      "Epoch 138 | Loss = 4.057063\n",
      "Epoch 139 | Loss = 3.992283\n",
      "Epoch 140 | Loss = 3.928630\n",
      "Epoch 141 | Loss = 3.866086\n",
      "Epoch 142 | Loss = 3.804635\n",
      "Epoch 143 | Loss = 3.744263\n",
      "Epoch 144 | Loss = 3.684953\n",
      "Epoch 145 | Loss = 3.626690\n",
      "Epoch 146 | Loss = 3.569456\n",
      "Epoch 147 | Loss = 3.513237\n",
      "Epoch 148 | Loss = 3.458017\n",
      "Epoch 149 | Loss = 3.403780\n",
      "Epoch 150 | Loss = 3.350511\n",
      "Epoch 151 | Loss = 3.298193\n",
      "Epoch 152 | Loss = 3.246812\n",
      "Epoch 153 | Loss = 3.196352\n",
      "Epoch 154 | Loss = 3.146798\n",
      "Epoch 155 | Loss = 3.098134\n",
      "Epoch 156 | Loss = 3.050347\n",
      "Epoch 157 | Loss = 3.003420\n",
      "Epoch 158 | Loss = 2.957341\n",
      "Epoch 159 | Loss = 2.912093\n",
      "Epoch 160 | Loss = 2.867663\n",
      "Epoch 161 | Loss = 2.824036\n",
      "Epoch 162 | Loss = 2.781199\n",
      "Epoch 163 | Loss = 2.739138\n",
      "Epoch 164 | Loss = 2.697839\n",
      "Epoch 165 | Loss = 2.657288\n",
      "Epoch 166 | Loss = 2.617473\n",
      "Epoch 167 | Loss = 2.578380\n",
      "Epoch 168 | Loss = 2.539997\n",
      "Epoch 169 | Loss = 2.502310\n",
      "Epoch 170 | Loss = 2.465308\n",
      "Epoch 171 | Loss = 2.428976\n",
      "Epoch 172 | Loss = 2.393305\n",
      "Epoch 173 | Loss = 2.358281\n",
      "Epoch 174 | Loss = 2.323892\n",
      "Epoch 175 | Loss = 2.290127\n",
      "Epoch 176 | Loss = 2.256975\n",
      "Epoch 177 | Loss = 2.224424\n",
      "Epoch 178 | Loss = 2.192463\n",
      "Epoch 179 | Loss = 2.161080\n",
      "Epoch 180 | Loss = 2.130266\n",
      "Epoch 181 | Loss = 2.100009\n",
      "Epoch 182 | Loss = 2.070300\n",
      "Epoch 183 | Loss = 2.041127\n",
      "Epoch 184 | Loss = 2.012480\n",
      "Epoch 185 | Loss = 1.984351\n",
      "Epoch 186 | Loss = 1.956728\n",
      "Epoch 187 | Loss = 1.929602\n",
      "Epoch 188 | Loss = 1.902964\n",
      "Epoch 189 | Loss = 1.876804\n",
      "Epoch 190 | Loss = 1.851113\n",
      "Epoch 191 | Loss = 1.825882\n",
      "Epoch 192 | Loss = 1.801102\n",
      "Epoch 193 | Loss = 1.776765\n",
      "Epoch 194 | Loss = 1.752861\n",
      "Epoch 195 | Loss = 1.729384\n",
      "Epoch 196 | Loss = 1.706323\n",
      "Epoch 197 | Loss = 1.683671\n",
      "Epoch 198 | Loss = 1.661420\n",
      "Epoch 199 | Loss = 1.639562\n",
      "Epoch 200 | Loss = 1.618090\n",
      "Epoch 201 | Loss = 1.596995\n",
      "Epoch 202 | Loss = 1.576271\n",
      "Epoch 203 | Loss = 1.555909\n",
      "Epoch 204 | Loss = 1.535904\n",
      "Epoch 205 | Loss = 1.516247\n",
      "Epoch 206 | Loss = 1.496932\n",
      "Epoch 207 | Loss = 1.477952\n",
      "Epoch 208 | Loss = 1.459301\n",
      "Epoch 209 | Loss = 1.440971\n",
      "Epoch 210 | Loss = 1.422956\n",
      "Epoch 211 | Loss = 1.405251\n",
      "Epoch 212 | Loss = 1.387849\n",
      "Epoch 213 | Loss = 1.370743\n",
      "Epoch 214 | Loss = 1.353928\n",
      "Epoch 215 | Loss = 1.337398\n",
      "Epoch 216 | Loss = 1.321148\n",
      "Epoch 217 | Loss = 1.305171\n",
      "Epoch 218 | Loss = 1.289463\n",
      "Epoch 219 | Loss = 1.274017\n",
      "Epoch 220 | Loss = 1.258829\n",
      "Epoch 221 | Loss = 1.243893\n",
      "Epoch 222 | Loss = 1.229204\n",
      "Epoch 223 | Loss = 1.214758\n",
      "Epoch 224 | Loss = 1.200549\n",
      "Epoch 225 | Loss = 1.186572\n",
      "Epoch 226 | Loss = 1.172824\n",
      "Epoch 227 | Loss = 1.159299\n",
      "Epoch 228 | Loss = 1.145993\n",
      "Epoch 229 | Loss = 1.132901\n",
      "Epoch 230 | Loss = 1.120020\n",
      "Epoch 231 | Loss = 1.107345\n",
      "Epoch 232 | Loss = 1.094871\n",
      "Epoch 233 | Loss = 1.082596\n",
      "Epoch 234 | Loss = 1.070514\n",
      "Epoch 235 | Loss = 1.058622\n",
      "Epoch 236 | Loss = 1.046917\n",
      "Epoch 237 | Loss = 1.035394\n",
      "Epoch 238 | Loss = 1.024049\n",
      "Epoch 239 | Loss = 1.012881\n",
      "Epoch 240 | Loss = 1.001884\n",
      "Epoch 241 | Loss = 0.991055\n",
      "Epoch 242 | Loss = 0.980392\n",
      "Epoch 243 | Loss = 0.969891\n",
      "Epoch 244 | Loss = 0.959548\n",
      "Epoch 245 | Loss = 0.949361\n",
      "Epoch 246 | Loss = 0.939326\n",
      "Epoch 247 | Loss = 0.929441\n",
      "Epoch 248 | Loss = 0.919703\n",
      "Epoch 249 | Loss = 0.910108\n",
      "Epoch 250 | Loss = 0.900655\n",
      "Epoch 251 | Loss = 0.891339\n",
      "Epoch 252 | Loss = 0.882159\n",
      "Epoch 253 | Loss = 0.873113\n",
      "Epoch 254 | Loss = 0.864196\n",
      "Epoch 255 | Loss = 0.855408\n",
      "Epoch 256 | Loss = 0.846745\n",
      "Epoch 257 | Loss = 0.838205\n",
      "Epoch 258 | Loss = 0.829786\n",
      "Epoch 259 | Loss = 0.821485\n",
      "Epoch 260 | Loss = 0.813301\n",
      "Epoch 261 | Loss = 0.805231\n",
      "Epoch 262 | Loss = 0.797272\n",
      "Epoch 263 | Loss = 0.789423\n",
      "Epoch 264 | Loss = 0.781682\n",
      "Epoch 265 | Loss = 0.774047\n",
      "Epoch 266 | Loss = 0.766516\n",
      "Epoch 267 | Loss = 0.759086\n",
      "Epoch 268 | Loss = 0.751756\n",
      "Epoch 269 | Loss = 0.744525\n",
      "Epoch 270 | Loss = 0.737390\n",
      "Epoch 271 | Loss = 0.730349\n",
      "Epoch 272 | Loss = 0.723402\n",
      "Epoch 273 | Loss = 0.716545\n",
      "Epoch 274 | Loss = 0.709778\n",
      "Epoch 275 | Loss = 0.703099\n",
      "Epoch 276 | Loss = 0.696507\n",
      "Epoch 277 | Loss = 0.689999\n",
      "Epoch 278 | Loss = 0.683575\n",
      "Epoch 279 | Loss = 0.677232\n",
      "Epoch 280 | Loss = 0.670970\n",
      "Epoch 281 | Loss = 0.664786\n",
      "Epoch 282 | Loss = 0.658681\n",
      "Epoch 283 | Loss = 0.652651\n",
      "Epoch 284 | Loss = 0.646697\n",
      "Epoch 285 | Loss = 0.640816\n",
      "Epoch 286 | Loss = 0.635008\n",
      "Epoch 287 | Loss = 0.629270\n",
      "Epoch 288 | Loss = 0.623603\n",
      "Epoch 289 | Loss = 0.618004\n",
      "Epoch 290 | Loss = 0.612473\n",
      "Epoch 291 | Loss = 0.607008\n",
      "Epoch 292 | Loss = 0.601609\n",
      "Epoch 293 | Loss = 0.596273\n",
      "Epoch 294 | Loss = 0.591001\n",
      "Epoch 295 | Loss = 0.585791\n",
      "Epoch 296 | Loss = 0.580642\n",
      "Epoch 297 | Loss = 0.575553\n",
      "Epoch 298 | Loss = 0.570523\n",
      "Epoch 299 | Loss = 0.565551\n",
      "Epoch 300 | Loss = 0.560637\n",
      "Epoch 301 | Loss = 0.555778\n",
      "Epoch 302 | Loss = 0.550975\n",
      "Epoch 303 | Loss = 0.546227\n",
      "Epoch 304 | Loss = 0.541532\n",
      "Epoch 305 | Loss = 0.536889\n",
      "Epoch 306 | Loss = 0.532299\n",
      "Epoch 307 | Loss = 0.527759\n",
      "Epoch 308 | Loss = 0.523270\n",
      "Epoch 309 | Loss = 0.518831\n",
      "Epoch 310 | Loss = 0.514440\n",
      "Epoch 311 | Loss = 0.510097\n",
      "Epoch 312 | Loss = 0.505801\n",
      "Epoch 313 | Loss = 0.501552\n",
      "Epoch 314 | Loss = 0.497348\n",
      "Epoch 315 | Loss = 0.493190\n",
      "Epoch 316 | Loss = 0.489076\n",
      "Epoch 317 | Loss = 0.485006\n",
      "Epoch 318 | Loss = 0.480979\n",
      "Epoch 319 | Loss = 0.476994\n",
      "Epoch 320 | Loss = 0.473052\n",
      "Epoch 321 | Loss = 0.469150\n",
      "Epoch 322 | Loss = 0.465289\n",
      "Epoch 323 | Loss = 0.461468\n",
      "Epoch 324 | Loss = 0.457687\n",
      "Epoch 325 | Loss = 0.453944\n",
      "Epoch 326 | Loss = 0.450240\n",
      "Epoch 327 | Loss = 0.446573\n",
      "Epoch 328 | Loss = 0.442944\n",
      "Epoch 329 | Loss = 0.439351\n",
      "Epoch 330 | Loss = 0.435795\n",
      "Epoch 331 | Loss = 0.432274\n",
      "Epoch 332 | Loss = 0.428788\n",
      "Epoch 333 | Loss = 0.425337\n",
      "Epoch 334 | Loss = 0.421920\n",
      "Epoch 335 | Loss = 0.418537\n",
      "Epoch 336 | Loss = 0.415187\n",
      "Epoch 337 | Loss = 0.411870\n",
      "Epoch 338 | Loss = 0.408585\n",
      "Epoch 339 | Loss = 0.405333\n",
      "Epoch 340 | Loss = 0.402111\n",
      "Epoch 341 | Loss = 0.398921\n",
      "Epoch 342 | Loss = 0.395762\n",
      "Epoch 343 | Loss = 0.392632\n",
      "Epoch 344 | Loss = 0.389533\n",
      "Epoch 345 | Loss = 0.386463\n",
      "Epoch 346 | Loss = 0.383422\n",
      "Epoch 347 | Loss = 0.380410\n",
      "Epoch 348 | Loss = 0.377426\n",
      "Epoch 349 | Loss = 0.374470\n",
      "Epoch 350 | Loss = 0.371542\n",
      "Epoch 351 | Loss = 0.368641\n",
      "Epoch 352 | Loss = 0.365767\n",
      "Epoch 353 | Loss = 0.362919\n",
      "Epoch 354 | Loss = 0.360098\n",
      "Epoch 355 | Loss = 0.357302\n",
      "Epoch 356 | Loss = 0.354533\n",
      "Epoch 357 | Loss = 0.351788\n",
      "Epoch 358 | Loss = 0.349069\n",
      "Epoch 359 | Loss = 0.346374\n",
      "Epoch 360 | Loss = 0.343703\n",
      "Epoch 361 | Loss = 0.341057\n",
      "Epoch 362 | Loss = 0.338434\n",
      "Epoch 363 | Loss = 0.335835\n",
      "Epoch 364 | Loss = 0.333259\n",
      "Epoch 365 | Loss = 0.330706\n",
      "Epoch 366 | Loss = 0.328176\n",
      "Epoch 367 | Loss = 0.325668\n",
      "Epoch 368 | Loss = 0.323182\n",
      "Epoch 369 | Loss = 0.320718\n",
      "Epoch 370 | Loss = 0.318276\n",
      "Epoch 371 | Loss = 0.315855\n",
      "Epoch 372 | Loss = 0.313456\n",
      "Epoch 373 | Loss = 0.311077\n",
      "Epoch 374 | Loss = 0.308719\n",
      "Epoch 375 | Loss = 0.306381\n",
      "Epoch 376 | Loss = 0.304063\n",
      "Epoch 377 | Loss = 0.301766\n",
      "Epoch 378 | Loss = 0.299488\n",
      "Epoch 379 | Loss = 0.297230\n",
      "Epoch 380 | Loss = 0.294991\n",
      "Epoch 381 | Loss = 0.292771\n",
      "Epoch 382 | Loss = 0.290570\n",
      "Epoch 383 | Loss = 0.288387\n",
      "Epoch 384 | Loss = 0.286224\n",
      "Epoch 385 | Loss = 0.284078\n",
      "Epoch 386 | Loss = 0.281951\n",
      "Epoch 387 | Loss = 0.279841\n",
      "Epoch 388 | Loss = 0.277749\n",
      "Epoch 389 | Loss = 0.275675\n",
      "Epoch 390 | Loss = 0.273618\n",
      "Epoch 391 | Loss = 0.271578\n",
      "Epoch 392 | Loss = 0.269555\n",
      "Epoch 393 | Loss = 0.267549\n",
      "Epoch 394 | Loss = 0.265560\n",
      "Epoch 395 | Loss = 0.263587\n",
      "Epoch 396 | Loss = 0.261630\n",
      "Epoch 397 | Loss = 0.259689\n",
      "Epoch 398 | Loss = 0.257765\n",
      "Epoch 399 | Loss = 0.255856\n",
      "Epoch 400 | Loss = 0.253963\n",
      "Epoch 401 | Loss = 0.252085\n",
      "Epoch 402 | Loss = 0.250222\n",
      "Epoch 403 | Loss = 0.248375\n",
      "Epoch 404 | Loss = 0.246543\n",
      "Epoch 405 | Loss = 0.244726\n",
      "Epoch 406 | Loss = 0.242923\n",
      "Epoch 407 | Loss = 0.241135\n",
      "Epoch 408 | Loss = 0.239361\n",
      "Epoch 409 | Loss = 0.237602\n",
      "Epoch 410 | Loss = 0.235857\n",
      "Epoch 411 | Loss = 0.234126\n",
      "Epoch 412 | Loss = 0.232408\n",
      "Epoch 413 | Loss = 0.230705\n",
      "Epoch 414 | Loss = 0.229015\n",
      "Epoch 415 | Loss = 0.227338\n",
      "Epoch 416 | Loss = 0.225675\n",
      "Epoch 417 | Loss = 0.224025\n",
      "Epoch 418 | Loss = 0.222389\n",
      "Epoch 419 | Loss = 0.220765\n",
      "Epoch 420 | Loss = 0.219154\n",
      "Epoch 421 | Loss = 0.217556\n",
      "Epoch 422 | Loss = 0.215971\n",
      "Epoch 423 | Loss = 0.214398\n",
      "Epoch 424 | Loss = 0.212837\n",
      "Epoch 425 | Loss = 0.211289\n",
      "Epoch 426 | Loss = 0.209753\n",
      "Epoch 427 | Loss = 0.208229\n",
      "Epoch 428 | Loss = 0.206716\n",
      "Epoch 429 | Loss = 0.205216\n",
      "Epoch 430 | Loss = 0.203728\n",
      "Epoch 431 | Loss = 0.202251\n",
      "Epoch 432 | Loss = 0.200785\n",
      "Epoch 433 | Loss = 0.199331\n",
      "Epoch 434 | Loss = 0.197889\n",
      "Epoch 435 | Loss = 0.196457\n",
      "Epoch 436 | Loss = 0.195037\n",
      "Epoch 437 | Loss = 0.193627\n",
      "Epoch 438 | Loss = 0.192229\n",
      "Epoch 439 | Loss = 0.190842\n",
      "Epoch 440 | Loss = 0.189465\n",
      "Epoch 441 | Loss = 0.188099\n",
      "Epoch 442 | Loss = 0.186743\n",
      "Epoch 443 | Loss = 0.185398\n",
      "Epoch 444 | Loss = 0.184063\n",
      "Epoch 445 | Loss = 0.182738\n",
      "Epoch 446 | Loss = 0.181424\n",
      "Epoch 447 | Loss = 0.180120\n",
      "Epoch 448 | Loss = 0.178825\n",
      "Epoch 449 | Loss = 0.177541\n",
      "Epoch 450 | Loss = 0.176266\n",
      "Epoch 451 | Loss = 0.175001\n",
      "Epoch 452 | Loss = 0.173746\n",
      "Epoch 453 | Loss = 0.172501\n",
      "Epoch 454 | Loss = 0.171265\n",
      "Epoch 455 | Loss = 0.170038\n",
      "Epoch 456 | Loss = 0.168821\n",
      "Epoch 457 | Loss = 0.167613\n",
      "Epoch 458 | Loss = 0.166414\n",
      "Epoch 459 | Loss = 0.165224\n",
      "Epoch 460 | Loss = 0.164044\n",
      "Epoch 461 | Loss = 0.162872\n",
      "Epoch 462 | Loss = 0.161709\n",
      "Epoch 463 | Loss = 0.160555\n",
      "Epoch 464 | Loss = 0.159410\n",
      "Epoch 465 | Loss = 0.158273\n",
      "Epoch 466 | Loss = 0.157145\n",
      "Epoch 467 | Loss = 0.156026\n",
      "Epoch 468 | Loss = 0.154915\n",
      "Epoch 469 | Loss = 0.153812\n",
      "Epoch 470 | Loss = 0.152718\n",
      "Epoch 471 | Loss = 0.151632\n",
      "Epoch 472 | Loss = 0.150554\n",
      "Epoch 473 | Loss = 0.149484\n",
      "Epoch 474 | Loss = 0.148422\n",
      "Epoch 475 | Loss = 0.147368\n",
      "Epoch 476 | Loss = 0.146322\n",
      "Epoch 477 | Loss = 0.145284\n",
      "Epoch 478 | Loss = 0.144254\n",
      "Epoch 479 | Loss = 0.143232\n",
      "Epoch 480 | Loss = 0.142217\n",
      "Epoch 481 | Loss = 0.141209\n",
      "Epoch 482 | Loss = 0.140210\n",
      "Epoch 483 | Loss = 0.139217\n",
      "Epoch 484 | Loss = 0.138233\n",
      "Epoch 485 | Loss = 0.137255\n",
      "Epoch 486 | Loss = 0.136285\n",
      "Epoch 487 | Loss = 0.135322\n",
      "Epoch 488 | Loss = 0.134366\n",
      "Epoch 489 | Loss = 0.133418\n",
      "Epoch 490 | Loss = 0.132476\n",
      "Epoch 491 | Loss = 0.131542\n",
      "Epoch 492 | Loss = 0.130614\n",
      "Epoch 493 | Loss = 0.129693\n",
      "Epoch 494 | Loss = 0.128779\n",
      "Epoch 495 | Loss = 0.127872\n",
      "Epoch 496 | Loss = 0.126972\n",
      "Epoch 497 | Loss = 0.126078\n",
      "Epoch 498 | Loss = 0.125191\n",
      "Epoch 499 | Loss = 0.124311\n",
      "Epoch 500 | Loss = 0.123437\n",
      "Epoch 501 | Loss = 0.122569\n",
      "Epoch 502 | Loss = 0.121708\n",
      "Epoch 503 | Loss = 0.120854\n",
      "Epoch 504 | Loss = 0.120005\n",
      "Epoch 505 | Loss = 0.119163\n",
      "Epoch 506 | Loss = 0.118328\n",
      "Epoch 507 | Loss = 0.117498\n",
      "Epoch 508 | Loss = 0.116674\n",
      "Epoch 509 | Loss = 0.115857\n",
      "Epoch 510 | Loss = 0.115046\n",
      "Epoch 511 | Loss = 0.114240\n",
      "Epoch 512 | Loss = 0.113441\n",
      "Epoch 513 | Loss = 0.112647\n",
      "Epoch 514 | Loss = 0.111859\n",
      "Epoch 515 | Loss = 0.111077\n",
      "Epoch 516 | Loss = 0.110301\n",
      "Epoch 517 | Loss = 0.109531\n",
      "Epoch 518 | Loss = 0.108766\n",
      "Epoch 519 | Loss = 0.108007\n",
      "Epoch 520 | Loss = 0.107253\n",
      "Epoch 521 | Loss = 0.106505\n",
      "Epoch 522 | Loss = 0.105763\n",
      "Epoch 523 | Loss = 0.105025\n",
      "Epoch 524 | Loss = 0.104294\n",
      "Epoch 525 | Loss = 0.103567\n",
      "Epoch 526 | Loss = 0.102846\n",
      "Epoch 527 | Loss = 0.102131\n",
      "Epoch 528 | Loss = 0.101420\n",
      "Epoch 529 | Loss = 0.100715\n",
      "Epoch 530 | Loss = 0.100015\n",
      "Epoch 531 | Loss = 0.099320\n",
      "Epoch 532 | Loss = 0.098630\n",
      "Epoch 533 | Loss = 0.097945\n",
      "Epoch 534 | Loss = 0.097265\n",
      "Epoch 535 | Loss = 0.096590\n",
      "Epoch 536 | Loss = 0.095920\n",
      "Epoch 537 | Loss = 0.095255\n",
      "Epoch 538 | Loss = 0.094595\n",
      "Epoch 539 | Loss = 0.093939\n",
      "Epoch 540 | Loss = 0.093289\n",
      "Epoch 541 | Loss = 0.092643\n",
      "Epoch 542 | Loss = 0.092002\n",
      "Epoch 543 | Loss = 0.091365\n",
      "Epoch 544 | Loss = 0.090733\n",
      "Epoch 545 | Loss = 0.090106\n",
      "Epoch 546 | Loss = 0.089483\n",
      "Epoch 547 | Loss = 0.088865\n",
      "Epoch 548 | Loss = 0.088252\n",
      "Epoch 549 | Loss = 0.087642\n",
      "Epoch 550 | Loss = 0.087038\n",
      "Epoch 551 | Loss = 0.086437\n",
      "Epoch 552 | Loss = 0.085841\n",
      "Epoch 553 | Loss = 0.085249\n",
      "Epoch 554 | Loss = 0.084662\n",
      "Epoch 555 | Loss = 0.084079\n",
      "Epoch 556 | Loss = 0.083500\n",
      "Epoch 557 | Loss = 0.082925\n",
      "Epoch 558 | Loss = 0.082355\n",
      "Epoch 559 | Loss = 0.081788\n",
      "Epoch 560 | Loss = 0.081226\n",
      "Epoch 561 | Loss = 0.080668\n",
      "Epoch 562 | Loss = 0.080113\n",
      "Epoch 563 | Loss = 0.079563\n",
      "Epoch 564 | Loss = 0.079017\n",
      "Epoch 565 | Loss = 0.078475\n",
      "Epoch 566 | Loss = 0.077936\n",
      "Epoch 567 | Loss = 0.077402\n",
      "Epoch 568 | Loss = 0.076871\n",
      "Epoch 569 | Loss = 0.076345\n",
      "Epoch 570 | Loss = 0.075822\n",
      "Epoch 571 | Loss = 0.075302\n",
      "Epoch 572 | Loss = 0.074787\n",
      "Epoch 573 | Loss = 0.074275\n",
      "Epoch 574 | Loss = 0.073767\n",
      "Epoch 575 | Loss = 0.073263\n",
      "Epoch 576 | Loss = 0.072762\n",
      "Epoch 577 | Loss = 0.072265\n",
      "Epoch 578 | Loss = 0.071771\n",
      "Epoch 579 | Loss = 0.071281\n",
      "Epoch 580 | Loss = 0.070795\n",
      "Epoch 581 | Loss = 0.070312\n",
      "Epoch 582 | Loss = 0.069832\n",
      "Epoch 583 | Loss = 0.069356\n",
      "Epoch 584 | Loss = 0.068884\n",
      "Epoch 585 | Loss = 0.068414\n",
      "Epoch 586 | Loss = 0.067949\n",
      "Epoch 587 | Loss = 0.067486\n",
      "Epoch 588 | Loss = 0.067027\n",
      "Epoch 589 | Loss = 0.066571\n",
      "Epoch 590 | Loss = 0.066118\n",
      "Epoch 591 | Loss = 0.065669\n",
      "Epoch 592 | Loss = 0.065223\n",
      "Epoch 593 | Loss = 0.064780\n",
      "Epoch 594 | Loss = 0.064340\n",
      "Epoch 595 | Loss = 0.063903\n",
      "Epoch 596 | Loss = 0.063469\n",
      "Epoch 597 | Loss = 0.063039\n",
      "Epoch 598 | Loss = 0.062612\n",
      "Epoch 599 | Loss = 0.062187\n",
      "Epoch 600 | Loss = 0.061766\n",
      "Epoch 601 | Loss = 0.061348\n",
      "Epoch 602 | Loss = 0.060932\n",
      "Epoch 603 | Loss = 0.060520\n",
      "Epoch 604 | Loss = 0.060111\n",
      "Epoch 605 | Loss = 0.059704\n",
      "Epoch 606 | Loss = 0.059301\n",
      "Epoch 607 | Loss = 0.058900\n",
      "Epoch 608 | Loss = 0.058502\n",
      "Epoch 609 | Loss = 0.058107\n",
      "Epoch 610 | Loss = 0.057715\n",
      "Epoch 611 | Loss = 0.057325\n",
      "Epoch 612 | Loss = 0.056939\n",
      "Epoch 613 | Loss = 0.056555\n",
      "Epoch 614 | Loss = 0.056174\n",
      "Epoch 615 | Loss = 0.055795\n",
      "Epoch 616 | Loss = 0.055420\n",
      "Epoch 617 | Loss = 0.055046\n",
      "Epoch 618 | Loss = 0.054676\n",
      "Epoch 619 | Loss = 0.054308\n",
      "Epoch 620 | Loss = 0.053943\n",
      "Epoch 621 | Loss = 0.053580\n",
      "Epoch 622 | Loss = 0.053220\n",
      "Epoch 623 | Loss = 0.052863\n",
      "Epoch 624 | Loss = 0.052508\n",
      "Epoch 625 | Loss = 0.052155\n",
      "Epoch 626 | Loss = 0.051805\n",
      "Epoch 627 | Loss = 0.051458\n",
      "Epoch 628 | Loss = 0.051113\n",
      "Epoch 629 | Loss = 0.050770\n",
      "Epoch 630 | Loss = 0.050430\n",
      "Epoch 631 | Loss = 0.050092\n",
      "Epoch 632 | Loss = 0.049757\n",
      "Epoch 633 | Loss = 0.049424\n",
      "Epoch 634 | Loss = 0.049093\n",
      "Epoch 635 | Loss = 0.048765\n",
      "Epoch 636 | Loss = 0.048439\n",
      "Epoch 637 | Loss = 0.048115\n",
      "Epoch 638 | Loss = 0.047794\n",
      "Epoch 639 | Loss = 0.047475\n",
      "Epoch 640 | Loss = 0.047158\n",
      "Epoch 641 | Loss = 0.046843\n",
      "Epoch 642 | Loss = 0.046531\n",
      "Epoch 643 | Loss = 0.046220\n",
      "Epoch 644 | Loss = 0.045912\n",
      "Epoch 645 | Loss = 0.045606\n",
      "Epoch 646 | Loss = 0.045303\n",
      "Epoch 647 | Loss = 0.045001\n",
      "Epoch 648 | Loss = 0.044701\n",
      "Epoch 649 | Loss = 0.044404\n",
      "Epoch 650 | Loss = 0.044109\n",
      "Epoch 651 | Loss = 0.043815\n",
      "Epoch 652 | Loss = 0.043524\n",
      "Epoch 653 | Loss = 0.043235\n",
      "Epoch 654 | Loss = 0.042948\n",
      "Epoch 655 | Loss = 0.042663\n",
      "Epoch 656 | Loss = 0.042380\n",
      "Epoch 657 | Loss = 0.042098\n",
      "Epoch 658 | Loss = 0.041819\n",
      "Epoch 659 | Loss = 0.041542\n",
      "Epoch 660 | Loss = 0.041267\n",
      "Epoch 661 | Loss = 0.040993\n",
      "Epoch 662 | Loss = 0.040722\n",
      "Epoch 663 | Loss = 0.040452\n",
      "Epoch 664 | Loss = 0.040185\n",
      "Epoch 665 | Loss = 0.039919\n",
      "Epoch 666 | Loss = 0.039655\n",
      "Epoch 667 | Loss = 0.039393\n",
      "Epoch 668 | Loss = 0.039132\n",
      "Epoch 669 | Loss = 0.038874\n",
      "Epoch 670 | Loss = 0.038617\n",
      "Epoch 671 | Loss = 0.038362\n",
      "Epoch 672 | Loss = 0.038109\n",
      "Epoch 673 | Loss = 0.037858\n",
      "Epoch 674 | Loss = 0.037608\n",
      "Epoch 675 | Loss = 0.037360\n",
      "Epoch 676 | Loss = 0.037114\n",
      "Epoch 677 | Loss = 0.036870\n",
      "Epoch 678 | Loss = 0.036627\n",
      "Epoch 679 | Loss = 0.036386\n",
      "Epoch 680 | Loss = 0.036147\n",
      "Epoch 681 | Loss = 0.035909\n",
      "Epoch 682 | Loss = 0.035673\n",
      "Epoch 683 | Loss = 0.035438\n",
      "Epoch 684 | Loss = 0.035206\n",
      "Epoch 685 | Loss = 0.034974\n",
      "Epoch 686 | Loss = 0.034745\n",
      "Epoch 687 | Loss = 0.034517\n",
      "Epoch 688 | Loss = 0.034290\n",
      "Epoch 689 | Loss = 0.034065\n",
      "Epoch 690 | Loss = 0.033842\n",
      "Epoch 691 | Loss = 0.033620\n",
      "Epoch 692 | Loss = 0.033400\n",
      "Epoch 693 | Loss = 0.033181\n",
      "Epoch 694 | Loss = 0.032964\n",
      "Epoch 695 | Loss = 0.032748\n",
      "Epoch 696 | Loss = 0.032534\n",
      "Epoch 697 | Loss = 0.032321\n",
      "Epoch 698 | Loss = 0.032110\n",
      "Epoch 699 | Loss = 0.031900\n",
      "Epoch 700 | Loss = 0.031692\n",
      "Epoch 701 | Loss = 0.031485\n",
      "Epoch 702 | Loss = 0.031279\n",
      "Epoch 703 | Loss = 0.031075\n",
      "Epoch 704 | Loss = 0.030873\n",
      "Epoch 705 | Loss = 0.030671\n",
      "Epoch 706 | Loss = 0.030471\n",
      "Epoch 707 | Loss = 0.030273\n",
      "Epoch 708 | Loss = 0.030076\n",
      "Epoch 709 | Loss = 0.029880\n",
      "Epoch 710 | Loss = 0.029685\n",
      "Epoch 711 | Loss = 0.029492\n",
      "Epoch 712 | Loss = 0.029300\n",
      "Epoch 713 | Loss = 0.029110\n",
      "Epoch 714 | Loss = 0.028920\n",
      "Epoch 715 | Loss = 0.028732\n",
      "Epoch 716 | Loss = 0.028546\n",
      "Epoch 717 | Loss = 0.028360\n",
      "Epoch 718 | Loss = 0.028176\n",
      "Epoch 719 | Loss = 0.027994\n",
      "Epoch 720 | Loss = 0.027812\n",
      "Epoch 721 | Loss = 0.027632\n",
      "Epoch 722 | Loss = 0.027452\n",
      "Epoch 723 | Loss = 0.027275\n",
      "Epoch 724 | Loss = 0.027098\n",
      "Epoch 725 | Loss = 0.026922\n",
      "Epoch 726 | Loss = 0.026748\n",
      "Epoch 727 | Loss = 0.026575\n",
      "Epoch 728 | Loss = 0.026403\n",
      "Epoch 729 | Loss = 0.026232\n",
      "Epoch 730 | Loss = 0.026063\n",
      "Epoch 731 | Loss = 0.025894\n",
      "Epoch 732 | Loss = 0.025727\n",
      "Epoch 733 | Loss = 0.025561\n",
      "Epoch 734 | Loss = 0.025396\n",
      "Epoch 735 | Loss = 0.025232\n",
      "Epoch 736 | Loss = 0.025069\n",
      "Epoch 737 | Loss = 0.024907\n",
      "Epoch 738 | Loss = 0.024747\n",
      "Epoch 739 | Loss = 0.024587\n",
      "Epoch 740 | Loss = 0.024429\n",
      "Epoch 741 | Loss = 0.024272\n",
      "Epoch 742 | Loss = 0.024115\n",
      "Epoch 743 | Loss = 0.023960\n",
      "Epoch 744 | Loss = 0.023806\n",
      "Epoch 745 | Loss = 0.023653\n",
      "Epoch 746 | Loss = 0.023501\n",
      "Epoch 747 | Loss = 0.023350\n",
      "Epoch 748 | Loss = 0.023200\n",
      "Epoch 749 | Loss = 0.023051\n",
      "Epoch 750 | Loss = 0.022903\n",
      "Epoch 751 | Loss = 0.022756\n",
      "Epoch 752 | Loss = 0.022610\n",
      "Epoch 753 | Loss = 0.022465\n",
      "Epoch 754 | Loss = 0.022321\n",
      "Epoch 755 | Loss = 0.022177\n",
      "Epoch 756 | Loss = 0.022035\n",
      "Epoch 757 | Loss = 0.021894\n",
      "Epoch 758 | Loss = 0.021754\n",
      "Epoch 759 | Loss = 0.021615\n",
      "Epoch 760 | Loss = 0.021476\n",
      "Epoch 761 | Loss = 0.021339\n",
      "Epoch 762 | Loss = 0.021203\n",
      "Epoch 763 | Loss = 0.021067\n",
      "Epoch 764 | Loss = 0.020932\n",
      "Epoch 765 | Loss = 0.020799\n",
      "Epoch 766 | Loss = 0.020666\n",
      "Epoch 767 | Loss = 0.020534\n",
      "Epoch 768 | Loss = 0.020403\n",
      "Epoch 769 | Loss = 0.020273\n",
      "Epoch 770 | Loss = 0.020143\n",
      "Epoch 771 | Loss = 0.020015\n",
      "Epoch 772 | Loss = 0.019887\n",
      "Epoch 773 | Loss = 0.019760\n",
      "Epoch 774 | Loss = 0.019635\n",
      "Epoch 775 | Loss = 0.019510\n",
      "Epoch 776 | Loss = 0.019385\n",
      "Epoch 777 | Loss = 0.019262\n",
      "Epoch 778 | Loss = 0.019139\n",
      "Epoch 779 | Loss = 0.019018\n",
      "Epoch 780 | Loss = 0.018897\n",
      "Epoch 781 | Loss = 0.018777\n",
      "Epoch 782 | Loss = 0.018657\n",
      "Epoch 783 | Loss = 0.018539\n",
      "Epoch 784 | Loss = 0.018421\n",
      "Epoch 785 | Loss = 0.018304\n",
      "Epoch 786 | Loss = 0.018188\n",
      "Epoch 787 | Loss = 0.018073\n",
      "Epoch 788 | Loss = 0.017958\n",
      "Epoch 789 | Loss = 0.017844\n",
      "Epoch 790 | Loss = 0.017731\n",
      "Epoch 791 | Loss = 0.017619\n",
      "Epoch 792 | Loss = 0.017507\n",
      "Epoch 793 | Loss = 0.017396\n",
      "Epoch 794 | Loss = 0.017286\n",
      "Epoch 795 | Loss = 0.017177\n",
      "Epoch 796 | Loss = 0.017068\n",
      "Epoch 797 | Loss = 0.016960\n",
      "Epoch 798 | Loss = 0.016853\n",
      "Epoch 799 | Loss = 0.016747\n",
      "Epoch 800 | Loss = 0.016641\n",
      "Epoch 801 | Loss = 0.016536\n",
      "Epoch 802 | Loss = 0.016431\n",
      "Epoch 803 | Loss = 0.016328\n",
      "Epoch 804 | Loss = 0.016225\n",
      "Epoch 805 | Loss = 0.016122\n",
      "Epoch 806 | Loss = 0.016021\n",
      "Epoch 807 | Loss = 0.015920\n",
      "Epoch 808 | Loss = 0.015819\n",
      "Epoch 809 | Loss = 0.015720\n",
      "Epoch 810 | Loss = 0.015621\n",
      "Epoch 811 | Loss = 0.015522\n",
      "Epoch 812 | Loss = 0.015425\n",
      "Epoch 813 | Loss = 0.015328\n",
      "Epoch 814 | Loss = 0.015231\n",
      "Epoch 815 | Loss = 0.015136\n",
      "Epoch 816 | Loss = 0.015040\n",
      "Epoch 817 | Loss = 0.014946\n",
      "Epoch 818 | Loss = 0.014852\n",
      "Epoch 819 | Loss = 0.014759\n",
      "Epoch 820 | Loss = 0.014666\n",
      "Epoch 821 | Loss = 0.014574\n",
      "Epoch 822 | Loss = 0.014483\n",
      "Epoch 823 | Loss = 0.014392\n",
      "Epoch 824 | Loss = 0.014302\n",
      "Epoch 825 | Loss = 0.014212\n",
      "Epoch 826 | Loss = 0.014123\n",
      "Epoch 827 | Loss = 0.014034\n",
      "Epoch 828 | Loss = 0.013947\n",
      "Epoch 829 | Loss = 0.013859\n",
      "Epoch 830 | Loss = 0.013773\n",
      "Epoch 831 | Loss = 0.013686\n",
      "Epoch 832 | Loss = 0.013601\n",
      "Epoch 833 | Loss = 0.013516\n",
      "Epoch 834 | Loss = 0.013431\n",
      "Epoch 835 | Loss = 0.013347\n",
      "Epoch 836 | Loss = 0.013264\n",
      "Epoch 837 | Loss = 0.013181\n",
      "Epoch 838 | Loss = 0.013099\n",
      "Epoch 839 | Loss = 0.013017\n",
      "Epoch 840 | Loss = 0.012936\n",
      "Epoch 841 | Loss = 0.012855\n",
      "Epoch 842 | Loss = 0.012775\n",
      "Epoch 843 | Loss = 0.012695\n",
      "Epoch 844 | Loss = 0.012616\n",
      "Epoch 845 | Loss = 0.012538\n",
      "Epoch 846 | Loss = 0.012460\n",
      "Epoch 847 | Loss = 0.012382\n",
      "Epoch 848 | Loss = 0.012305\n",
      "Epoch 849 | Loss = 0.012229\n",
      "Epoch 850 | Loss = 0.012152\n",
      "Epoch 851 | Loss = 0.012077\n",
      "Epoch 852 | Loss = 0.012002\n",
      "Epoch 853 | Loss = 0.011927\n",
      "Epoch 854 | Loss = 0.011853\n",
      "Epoch 855 | Loss = 0.011780\n",
      "Epoch 856 | Loss = 0.011706\n",
      "Epoch 857 | Loss = 0.011634\n",
      "Epoch 858 | Loss = 0.011562\n",
      "Epoch 859 | Loss = 0.011490\n",
      "Epoch 860 | Loss = 0.011419\n",
      "Epoch 861 | Loss = 0.011348\n",
      "Epoch 862 | Loss = 0.011278\n",
      "Epoch 863 | Loss = 0.011208\n",
      "Epoch 864 | Loss = 0.011138\n",
      "Epoch 865 | Loss = 0.011069\n",
      "Epoch 866 | Loss = 0.011001\n",
      "Epoch 867 | Loss = 0.010933\n",
      "Epoch 868 | Loss = 0.010865\n",
      "Epoch 869 | Loss = 0.010798\n",
      "Epoch 870 | Loss = 0.010731\n",
      "Epoch 871 | Loss = 0.010665\n",
      "Epoch 872 | Loss = 0.010599\n",
      "Epoch 873 | Loss = 0.010533\n",
      "Epoch 874 | Loss = 0.010468\n",
      "Epoch 875 | Loss = 0.010404\n",
      "Epoch 876 | Loss = 0.010340\n",
      "Epoch 877 | Loss = 0.010276\n",
      "Epoch 878 | Loss = 0.010212\n",
      "Epoch 879 | Loss = 0.010149\n",
      "Epoch 880 | Loss = 0.010087\n",
      "Epoch 881 | Loss = 0.010025\n",
      "Epoch 882 | Loss = 0.009963\n",
      "Epoch 883 | Loss = 0.009902\n",
      "Epoch 884 | Loss = 0.009841\n",
      "Epoch 885 | Loss = 0.009780\n",
      "Epoch 886 | Loss = 0.009720\n",
      "Epoch 887 | Loss = 0.009660\n",
      "Epoch 888 | Loss = 0.009601\n",
      "Epoch 889 | Loss = 0.009542\n",
      "Epoch 890 | Loss = 0.009483\n",
      "Epoch 891 | Loss = 0.009425\n",
      "Epoch 892 | Loss = 0.009367\n",
      "Epoch 893 | Loss = 0.009310\n",
      "Epoch 894 | Loss = 0.009252\n",
      "Epoch 895 | Loss = 0.009196\n",
      "Epoch 896 | Loss = 0.009139\n",
      "Epoch 897 | Loss = 0.009083\n",
      "Epoch 898 | Loss = 0.009027\n",
      "Epoch 899 | Loss = 0.008972\n",
      "Epoch 900 | Loss = 0.008917\n",
      "Epoch 901 | Loss = 0.008863\n",
      "Epoch 902 | Loss = 0.008808\n",
      "Epoch 903 | Loss = 0.008754\n",
      "Epoch 904 | Loss = 0.008701\n",
      "Epoch 905 | Loss = 0.008648\n",
      "Epoch 906 | Loss = 0.008595\n",
      "Epoch 907 | Loss = 0.008542\n",
      "Epoch 908 | Loss = 0.008490\n",
      "Epoch 909 | Loss = 0.008438\n",
      "Epoch 910 | Loss = 0.008387\n",
      "Epoch 911 | Loss = 0.008335\n",
      "Epoch 912 | Loss = 0.008284\n",
      "Epoch 913 | Loss = 0.008234\n",
      "Epoch 914 | Loss = 0.008184\n",
      "Epoch 915 | Loss = 0.008134\n",
      "Epoch 916 | Loss = 0.008084\n",
      "Epoch 917 | Loss = 0.008035\n",
      "Epoch 918 | Loss = 0.007986\n",
      "Epoch 919 | Loss = 0.007937\n",
      "Epoch 920 | Loss = 0.007889\n",
      "Epoch 921 | Loss = 0.007841\n",
      "Epoch 922 | Loss = 0.007793\n",
      "Epoch 923 | Loss = 0.007746\n",
      "Epoch 924 | Loss = 0.007699\n",
      "Epoch 925 | Loss = 0.007652\n",
      "Epoch 926 | Loss = 0.007605\n",
      "Epoch 927 | Loss = 0.007559\n",
      "Epoch 928 | Loss = 0.007513\n",
      "Epoch 929 | Loss = 0.007467\n",
      "Epoch 930 | Loss = 0.007422\n",
      "Epoch 931 | Loss = 0.007377\n",
      "Epoch 932 | Loss = 0.007332\n",
      "Epoch 933 | Loss = 0.007288\n",
      "Epoch 934 | Loss = 0.007244\n",
      "Epoch 935 | Loss = 0.007200\n",
      "Epoch 936 | Loss = 0.007156\n",
      "Epoch 937 | Loss = 0.007113\n",
      "Epoch 938 | Loss = 0.007070\n",
      "Epoch 939 | Loss = 0.007027\n",
      "Epoch 940 | Loss = 0.006984\n",
      "Epoch 941 | Loss = 0.006942\n",
      "Epoch 942 | Loss = 0.006900\n",
      "Epoch 943 | Loss = 0.006858\n",
      "Epoch 944 | Loss = 0.006817\n",
      "Epoch 945 | Loss = 0.006776\n",
      "Epoch 946 | Loss = 0.006735\n",
      "Epoch 947 | Loss = 0.006694\n",
      "Epoch 948 | Loss = 0.006653\n",
      "Epoch 949 | Loss = 0.006613\n",
      "Epoch 950 | Loss = 0.006573\n",
      "Epoch 951 | Loss = 0.006534\n",
      "Epoch 952 | Loss = 0.006494\n",
      "Epoch 953 | Loss = 0.006455\n",
      "Epoch 954 | Loss = 0.006416\n",
      "Epoch 955 | Loss = 0.006378\n",
      "Epoch 956 | Loss = 0.006339\n",
      "Epoch 957 | Loss = 0.006301\n",
      "Epoch 958 | Loss = 0.006263\n",
      "Epoch 959 | Loss = 0.006225\n",
      "Epoch 960 | Loss = 0.006188\n",
      "Epoch 961 | Loss = 0.006151\n",
      "Epoch 962 | Loss = 0.006114\n",
      "Epoch 963 | Loss = 0.006077\n",
      "Epoch 964 | Loss = 0.006040\n",
      "Epoch 965 | Loss = 0.006004\n",
      "Epoch 966 | Loss = 0.005968\n",
      "Epoch 967 | Loss = 0.005932\n",
      "Epoch 968 | Loss = 0.005896\n",
      "Epoch 969 | Loss = 0.005861\n",
      "Epoch 970 | Loss = 0.005826\n",
      "Epoch 971 | Loss = 0.005791\n",
      "Epoch 972 | Loss = 0.005756\n",
      "Epoch 973 | Loss = 0.005722\n",
      "Epoch 974 | Loss = 0.005687\n",
      "Epoch 975 | Loss = 0.005653\n",
      "Epoch 976 | Loss = 0.005619\n",
      "Epoch 977 | Loss = 0.005586\n",
      "Epoch 978 | Loss = 0.005552\n",
      "Epoch 979 | Loss = 0.005519\n",
      "Epoch 980 | Loss = 0.005486\n",
      "Epoch 981 | Loss = 0.005453\n",
      "Epoch 982 | Loss = 0.005421\n",
      "Epoch 983 | Loss = 0.005388\n",
      "Epoch 984 | Loss = 0.005356\n",
      "Epoch 985 | Loss = 0.005324\n",
      "Epoch 986 | Loss = 0.005292\n",
      "Epoch 987 | Loss = 0.005261\n",
      "Epoch 988 | Loss = 0.005229\n",
      "Epoch 989 | Loss = 0.005198\n",
      "Epoch 990 | Loss = 0.005167\n",
      "Epoch 991 | Loss = 0.005136\n",
      "Epoch 992 | Loss = 0.005106\n",
      "Epoch 993 | Loss = 0.005075\n",
      "Epoch 994 | Loss = 0.005045\n",
      "Epoch 995 | Loss = 0.005015\n",
      "Epoch 996 | Loss = 0.004985\n",
      "Epoch 997 | Loss = 0.004955\n",
      "Epoch 998 | Loss = 0.004926\n",
      "Epoch 999 | Loss = 0.004897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pymatcha.tensor import Tensor\n",
    "from pymatcha.nn import Linear, ReLU\n",
    "from pymatcha.optim import MSELoss, SGD\n",
    "\n",
    "# -------------------------\n",
    "# 测试训练小网络\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    # 模拟数据\n",
    "    X = Tensor(np.random.randn(100, 20), requires_grad=False)\n",
    "    # 假设目标 y = X*2 + 1 (简化)\n",
    "    true_w = np.random.randn(20, 10)\n",
    "    y_true = Tensor(X.data @ true_w + 1, requires_grad=False)\n",
    "\n",
    "    # 小型网络\n",
    "    fc1 = Linear(20, 64)\n",
    "    fc2 = Linear(64, 10)\n",
    "    fc3 = ReLU()\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(fc1.parameters() + fc2.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        # 前向\n",
    "        h = fc1(X)\n",
    "        # j = fc3(h)\n",
    "        y_pred = fc2(h)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Loss = {loss.data:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4bca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss = 2.302023\n",
      "Epoch 010 | Loss = 2.301230\n",
      "Epoch 020 | Loss = 2.300453\n",
      "Epoch 030 | Loss = 2.299692\n",
      "Epoch 040 | Loss = 2.298945\n",
      "Epoch 050 | Loss = 2.298214\n",
      "Epoch 060 | Loss = 2.297496\n",
      "Epoch 070 | Loss = 2.296794\n",
      "Epoch 080 | Loss = 2.296105\n",
      "Epoch 090 | Loss = 2.295430\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pymatcha.tensor import Tensor\n",
    "from pymatcha.nn import Conv2d, Linear, ReLU, Softmax\n",
    "from pymatcha.optim import SGD, CrossEntropyLoss\n",
    "\n",
    "# -------------------------\n",
    "# 测试训练小型卷积网络（分类任务）\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. 构造“图片分类”数据集\n",
    "    # -------------------------\n",
    "    num_samples = 200      # 样本数\n",
    "    num_classes = 10       # 类别数\n",
    "    img_size = 8           # 图片大小 8x8\n",
    "    in_channels = 3        # 输入通道（3 通道彩色图像）\n",
    "\n",
    "    # 输入图像 (N, C, H, W)\n",
    "    X_data = np.random.randn(num_samples, in_channels, img_size, img_size)\n",
    "\n",
    "    # 随机生成权重并计算“真实 logits”\n",
    "    true_w = np.random.randn(in_channels * img_size * img_size, num_classes)\n",
    "    logits = X_data.reshape(num_samples, -1) @ true_w\n",
    "\n",
    "    # 用 argmax 生成标签\n",
    "    y_label = np.argmax(logits, axis=1)\n",
    "\n",
    "    # 转成 Tensor\n",
    "    X = Tensor(X_data, requires_grad=False)\n",
    "    y_true = Tensor(y_label, requires_grad=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. 定义卷积网络结构\n",
    "    # -------------------------\n",
    "    conv1 = Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "    relu1 = ReLU()\n",
    "    conv2 = Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "    relu2 = ReLU()\n",
    "\n",
    "    # Flatten + 全连接层\n",
    "    fc = Linear(16 * img_size * img_size, num_classes)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(conv1.parameters() + conv2.parameters() + fc.parameters(), lr=0.01)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. 训练循环\n",
    "    # -------------------------\n",
    "    for epoch in range(100):\n",
    "        # 前向传播\n",
    "        out1 = relu1(conv1(X))\n",
    "        out2 = relu2(conv2(out1))\n",
    "\n",
    "        # 展平\n",
    "        flat = Tensor(out2.data.reshape(num_samples, -1), requires_grad=True)\n",
    "        y_pred = softmax(fc(flat))\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | Loss = {loss.data:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ff2b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss = 2.302709\n",
      "Epoch 010 | Loss = 2.301909\n",
      "Epoch 020 | Loss = 2.301123\n",
      "Epoch 030 | Loss = 2.300354\n",
      "Epoch 040 | Loss = 2.299600\n",
      "Epoch 050 | Loss = 2.298861\n",
      "Epoch 060 | Loss = 2.298136\n",
      "Epoch 070 | Loss = 2.297426\n",
      "Epoch 080 | Loss = 2.296730\n",
      "Epoch 090 | Loss = 2.296048\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pymatcha.tensor import Tensor\n",
    "from pymatcha.nn import Conv2d, Linear, ReLU, Softmax, MaxPool2d, AvgPool2d\n",
    "from pymatcha.optim import SGD, CrossEntropyLoss\n",
    "\n",
    "# -------------------------\n",
    "# 测试训练小型卷积网络（分类任务）\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. 构造“图片分类”数据集\n",
    "    # -------------------------\n",
    "    num_samples = 200       # 样本数\n",
    "    num_classes = 10        # 类别数\n",
    "    img_size = 8            # 图片大小 8x8\n",
    "    in_channels = 3         # 输入通道（RGB）\n",
    "\n",
    "    # 输入图像 (N, C, H, W)\n",
    "    X_data = np.random.randn(num_samples, in_channels, img_size, img_size)\n",
    "\n",
    "    # 构造伪标签\n",
    "    true_w = np.random.randn(in_channels * img_size * img_size, num_classes)\n",
    "    logits = X_data.reshape(num_samples, -1) @ true_w\n",
    "    y_label = np.argmax(logits, axis=1)\n",
    "\n",
    "    X = Tensor(X_data, requires_grad=False)\n",
    "    y_true = Tensor(y_label, requires_grad=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. 定义卷积网络结构\n",
    "    # -------------------------\n",
    "    conv1 = Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "    relu1 = ReLU()\n",
    "    pool1 = MaxPool2d(kernel_size=2, stride=2)   # 最大池化\n",
    "\n",
    "    conv2 = Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "    relu2 = ReLU()\n",
    "    pool2 = AvgPool2d(kernel_size=2, stride=2)   # 平均池化\n",
    "\n",
    "    # 计算卷积+池化后的特征图尺寸\n",
    "    out_size = img_size // 2 // 2  # 两次池化各减半\n",
    "    fc = Linear(16 * out_size * out_size, num_classes)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(conv1.parameters() + conv2.parameters() + fc.parameters(), lr=0.01)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. 训练循环\n",
    "    # -------------------------\n",
    "    for epoch in range(100):\n",
    "        # 前向传播\n",
    "        out1 = pool1(relu1(conv1(X)))  # Conv -> ReLU -> MaxPool\n",
    "        out2 = pool2(relu2(conv2(out1)))  # Conv -> ReLU -> AvgPool\n",
    "\n",
    "        # 展平\n",
    "        flat = Tensor(out2.data.reshape(num_samples, -1), requires_grad=True)\n",
    "        y_pred = softmax(fc(flat))\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # 反向传播 + 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | Loss = {loss.data:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace05907",
   "metadata": {},
   "source": [
    "[Micrograd](https://github.com/karpathy/micrograd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymatcha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
